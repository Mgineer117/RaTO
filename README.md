Below is an augmented version of the report with an added experimental remark that emphasizes our observation in practice. The new remark is integrated as an additional section with a detailed explanation.

---

## 1. Background and Motivation

In reinforcement learning (RL), the goal is to learn policies that minimize long‐term cost (or maximize reward) in a sequential decision-making problem. A central object in RL is the value function, which captures the expected cumulative cost (or reward) when starting from a given state and following a policy. In many control problems, particularly when ensuring stability is critical, it is useful to have a Lyapunov function—a scalar function that decreases along trajectories—to certify stability. Under suitable conditions, the value function itself can serve as a Lyapunov function.

---

## 2. Value Function in Reinforcement Learning

Consider a Markov decision process (MDP) with state space \(\mathcal{S}\), action space \(\mathcal{A}\), a transition function \(f\) (or probability transition kernel \(P\)), and an immediate cost function \(r(x,a)\). For a given policy \(\pi\), the value function \(v(x)\) is defined as

\[
v(x) = r(x, \pi(x)) + \gamma\, v\big(f(x, \pi(x))\big),
\]

where \(\gamma \in (0,1)\) is the discount factor (or \(\gamma=1\) in an average-cost formulation under proper conditions). Notice that if the cost \(r(x, \pi(x))\) is strictly positive away from the origin—say, if \(r(x, \pi(x)) > 0\) for all \(x \neq 0\) and \(r(0,\pi(0)) = 0\)—this recursive definition naturally imposes a decrease in the value function along the system’s trajectory.

---

## 3. Lyapunov Functions in Stability Analysis

A Lyapunov function \(W(x)\) for a system is a positive-definite function that decreases along trajectories. For a discrete-time system 
\[
x_{k+1} = f(x_k, u_k),
\]
a candidate Lyapunov function \(W(x)\) must satisfy:
1. **Positivity:** \(W(x) > 0\) for all \(x \neq 0\) and \(W(0)=0\).
2. **Descent:** \(W(x_{k+1}) - W(x_k) < 0\) for all \(x \neq 0\).

When these conditions hold, \(W(x)\) certifies that the system is (locally or globally) asymptotically stable.

---

## 4. Establishing the Connection

### 4.1. Contraction and Descent

The Bellman operator used in value iteration is a contraction mapping:
\[
v_{k+1}(x) = \min_{a\in\mathcal{A}}\Big\{ r(x,a) + \gamma\, v_k\big(f(x,a)\big) \Big\},
\]
which guarantees that the successive value estimates converge to the unique fixed point \(v^*(x)\). This contraction property is analogous to a Lyapunov descent condition in function space, as the “distance” from the fixed point decreases with each iteration.

### 4.2. Value Function as a Lyapunov Candidate

Assume we work in a cost-minimization framework where the cost \(r(x,\pi(x))\) is strictly positive for all \(x \neq 0\) and zero at the origin. Then the value function (possibly normalized so that \(v(0)=0\)) satisfies:
\[
v(x) = r(x, \pi(x)) + v\big(f(x,\pi(x))\big).
\]
Because \(r(x, \pi(x)) > 0\) when \(x \neq 0\), it immediately follows that
\[
v(x) > v\big(f(x,\pi(x))\big) \quad \text{for } x \neq 0.
\]
Thus, along trajectories generated by the policy \(\pi\), the value function decreases. This is precisely the condition required for a Lyapunov function: it demonstrates a monotonic decrease toward the equilibrium. In this sense, the optimal (or approximate) value function naturally acts as a Lyapunov function.

---

## 5. Experimental Remark: Exploiting Value Functions as Lyapunov Functions

**Remark:**  
_In our experiments, we exploit the fact that value functions in RL are Lyapunov functions if the costs are strictly positive away from the origin. This follows directly from the definition of the value function. Specifically, if we have_
\[
v(x) = r(x, \pi(x)) + v\big(f(x,\pi(x))\big),
\]
_and if \(r(x, \pi(x)) > 0\) for all \(x \neq 0\), then it must be that_
\[
v(x) > v\big(f(x,\pi(x))\big) \quad \text{for all } x \neq 0.
\]
_This inequality implies that the value function decreases along system trajectories, making it a valid Lyapunov candidate. In practice, this property allows us to use the value function computed via approximate dynamic programming as a tool for certifying stability and ensuring safety, without the need to separately construct a Lyapunov function. This “by-product” of the RL algorithm is particularly valuable in settings such as safe RL, where ensuring that the system remains stable (or safe) throughout learning is critical._

**Explanation:**  
- **Strictly Positive Costs:** When the immediate cost \(r(x, \pi(x))\) is strictly positive for all nonzero states, every transition under the policy incurs a positive “penalty” unless the system is at the origin.
- **Recursive Decrease:** The recursive nature of the value function implies that the current cost plus the expected future cost must be higher than the future cost alone. Hence, the value at the current state is strictly higher than at the next state.
- **Stability Certification:** This descent property mirrors the classical Lyapunov condition used in control theory to certify stability. Therefore, by simply computing the value function, we obtain a function that can serve to assess (and even enforce) stability along trajectories.
- **Practical Benefit:** In our experiments, this observation enables us to extract a Lyapunov candidate “for free” from the approximate dynamic programming process. It streamlines the integration of safety constraints into RL by leveraging the inherent properties of the value function, reducing the need for additional design efforts to obtain a Lyapunov function.

---

## 6. Conclusion

Under appropriate conditions—specifically, when the immediate costs are strictly positive away from the origin—the value function in reinforcement learning not only guides optimal decision-making but also inherently satisfies the properties of a Lyapunov function. This dual role provides a powerful tool in both theoretical analyses and practical implementations, enabling safe and stable policy learning as a natural by-product of approximate dynamic programming. The experimental observation that the value function decreases along trajectories underpins its use as a Lyapunov candidate in our work.

---

Would you like further details on any specific aspect of these derivations or examples from our experiments?
